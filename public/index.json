[
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Nguyen Truong Huy.\nPhone Number: 0827338992.\nEmail: truonghuy20203@gmail.com .\nUniversity: FPT University.\nMajor: Information System.\nClass: AWS082025.\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern.\nInternship Duration: From September to December 2025.\nReport Content: Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Connect and get acquainted with members of the First Cloud Journey (FCJ).\nUnderstand basic AWS services.\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get to know FCJ members. - Read and note the rules and regulations of the internship unit. 08/09/2025 09/09/2025 3 - Learn about Cloud Computing. 09/09/2025 10/09/2025 \u0026lt;Youtube: AWS Study Group\u0026gt; 4 - Study at the office. - Continue learning about Cloud Computing. - First random blog translation attempt. 10/09/2025 11/09/2025 \u0026lt;Youtube: AWS Study Group\u0026gt; 5 - Learn about basic VPC concepts: + Subnet. + Route Table. + Security. - Watch labs 1 on AWS. 11/09/2025 13/09/2025 \u0026lt;Youtube: AWS Study Group\u0026gt; 6 - Continue learning about Cloud Computing on YouTube. 12/09/2025 14/09/2025 \u0026lt;Youtube: AWS Study Group\u0026gt; Week 1 Achievements Monday (08/09/2025):\nConnected and got acquainted with members of the First Cloud Journey (FCJ).\nRead, took notes, and understood the rules and regulations of the internship unit.\nTuesday (09/09/2025):\nStudied and understood the basic concepts of Cloud Computing. Wednesday (10/09/2025):\nGo to the office for studying.\nAttended the internship office and got familiar with the working environment.\nContinued learning about Cloud Computing.\nFirst random AWS blog translation attempt.\nThursday (11/09/2025):\nLearned about VPC (Virtual Private Cloud) and its fundamental components:\nSubnet.\nRoute Table.\nSecurity Group.\nWatched and took notes on AWS Lab 1 about network configuration.\nFriday (12/09/2025):\nContinued learning about Cloud Computing through YouTube.\nUnderstanding of cloud infrastructure operations and AWS benefits.\n"
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Practice using AWS technologies.\nUnderstand basic AWS services.\nTasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 - Continue learning AWS on YouTube. - Explore and understand AWS technologies. 15/09/2025 15/09/2025 3 - Attend a team meeting to discuss project ideas, programming languages, and technologies to be used. 16/09/2025 16/09/2025 4 - Learn AWS through YouTube videos. 17/09/2025 17/09/2025 \u0026lt;Youtube: AWS Study Group\u0026gt; 5 - Attend the AWS Cloud Day Vietnam event at the office. 18/09/2025 18/09/2025 \u0026lt;Youtube: AWS Study Group\u0026gt; 6 - Learn how to use AWS technologies. 19/09/2025 21/09/2025 \u0026lt;Youtube: AWS Study Group\u0026gt; Week 2 Achievements: Monday (15/09/2025):\nContinued learning AWS on YouTube, reinforcing basic concepts of cloud computing.\nExplored core AWS technologies and services, such as Compute, Storage, Database, and Networking.\nTuesday (16/09/2025):\nParticipated in a team meeting to discuss project ideas.\nAgreed on the programming language, technologies, and AWS services to be used during project development.\nWednesday (17/09/2025):\nContinued studying AWS content through instructional videos.\nBecame familiar with the AWS Management Console and how to access different services.\nThursday (18/09/2025):\nAttended the AWS Cloud Day Vietnam 2025 event at the office.\nListened to AWS experts discuss Cloud Computing trends, AI/ML, and Digital Transformation in businesses.\nFriday (19/09/2025):\nLearned how to use and work with AWS technologies.\nBegan getting familiar with deploying and managing basic AWS services.\n"
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Understand how to use AWS technologies.\nLearn basic AWS services, including how to use the Console and CLI.\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn AWS through YouTube. 22/09/2025 22/09/2025 3 - Try to translate the second random blog. 23/09/2025 23/09/2025 Youtube: AWS Cloud Journey 4 - Learn about Compute VM on AWS. - Study EC2 Instance Types - Including: + Amazon Elastic Compute Cloud (EC2). + Amazon Lightsail. 24/09/2025 24/09/2025 https://aws.amazon.com/ec2/instance-types/?ncl=h_is/ 5 - Study basic EC2 Concepts:\n+ Amazon EFS/FSX. + AWS Application Migration Service (MGN). 25/09/2025 27/09/2025 https://aws.amazon.com/ec2/instance-types/?ncl=h_is/ 6 - Practice: + Create an EC2 instance. + Create a database. 26/09/2025 28/09/2025 Week 3 Achievements: Monday (22/09/2025):\nLearn the basic concepts of AWS through YouTube tutorials.\nGained an overview of AWS core services.\nTuesday (23/09/2025):\nCompleted translation of the second blog. Wednesday (24/09/2025):\nStudied Compute VM on AWS.\nExplored the Compute service group on AWS, including:\nAmazon EC2.\nAmazon Lightsail.\nThursday (25/09/2025):\nUnderstanding of Basic EC2 concepts.\nLearned how to choose the right instance type.\nStudied the following services:\nAmazon EFS / FSX.\n**AWS Application Migration Service (MGN).\n→ Gained comprehensive knowledge of EC2 instance types.\nFriday (26/09/2025):\nSuccessfully completed hands-on practice:\nCreated an EC2 instance.\nCreated a database on AWS.\nBecame familiar with using the AWS Management Console.\n"
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Create an AWS account.\nPractice using AWS modules and core services.\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Create an AWS account. - Successfully activate $200 free credits. 29/10/2025 29/10/2025 [https://us-east-2.console.aws.amazon.com/console/home?/] 3 - Perform basic operations on AWS: + Create an EC2 Instance. + Learn about Billing \u0026amp; Cost Management.\n+ Explore Aurora \u0026amp; RDS. 30/10/2025 01/10/2025 [https://us-east-2.console.aws.amazon.com/console/home?/] 4 - Explore and practice with EC2. - Get familiar with Billing and Cost Management. - Install and configure AWS CLI for basic resource management. 01/10/2025 02/10/2025 [https://us-east-2.console.aws.amazon.com/console/home?/] 5 - Learn about AWS Private Certificate Authority. - Understand how to manage Databases on AWS. - Translate 3 assigned blog post. 02/10/2025 02/10/2025 [https://us-east-2.console.aws.amazon.com/console/home?/] 6 - Study AWS Lambda and Amazon Bedrock. - Practice deploying serverless services. - Review basic operations on Console \u0026amp; CLI. - Successfully push the project folder to GitHub. 03/10/2025 05/10/2025 [https://us-east-2.console.aws.amazon.com/console/home?/] Week 4 Achievements Monday (29/09/2025):\nSuccessfully created an AWS Free Tier account.\nCompleted activation of the $200 free credit.\nTuesday (30/09/2025):\nLaunched and operated an EC2 Instance.\nBecame familiar with Billing \u0026amp; Cost Management.\nWednesday (01/10/2025):\nPracticed more deeply with EC2.\nUnderstood and used key features in Billing and Cost Management.\nInstalled and configured AWS CLI for managing basic resources.\nThursday (02/10/2025):\nLearned about AWS Private Certificate Authority.\nBecame familiar with database management in AWS.\nTranslate 3 assigned blog post.\nFriday (03/10/2025):\nStudied and practiced with AWS Lambda and Amazon Bedrock.\nExperimented with serverless deployment.\nPracticed essential operations on Console \u0026amp; CLI.\nSuccessfully pushed the project folder to GitHub.\n"
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Continue learning AWS on YouTube.\nUnderstand core AWS services.\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study at the office. - Learn Module 03: Amazon Elastic Compute Cloud (EC2): + AMI / Backup / Key Pair. + Elastic block store. + User data. + Meta data. 06/10/2025 06/10/2025 Youtube: AWS Cloud Journey 3 - Continue studying Amazon EC2: + EC2 auto scaling. + EFS/FSX. + Lightsail. + MGN. 07/10/2025 07/10/2025 Youtube: AWS Cloud Journey 4 - Study at the office. - Translate and edit AWS blog content. 08/10/2025 08/10/2025 Youtube: AWS Cloud Journey 5 - Study AWS Storage Services: + Amazon Simple Storage. Service - S3. + Amazon Storage Gateway. + Snow Family. + Disaster Recovery on AWS. + AWS Backup. - Complete and review translated blog. 09/10/2025 11/10/2025 Youtube: AWS Cloud Journey 6 - Continue Module 04: + Access Point. + Storage Class. + S3 Static Website \u0026amp; CORS. + Control Access. + Object Key \u0026amp; Performance. + Glacier. 10/10/2025 12/10/2025 Youtube: AWS Cloud Journey Week 5 Achievements Monday (06/10/2025):\nGo to the office for studying.\nLearned and practiced with Amazon EC2: created and configured EC2 instances.\nPracticed AMI, EBS, User Data, and Meta Data.\nGenerated and used Key Pairs for secure access to instances.\nTuesday (07/10/2025):\nGained knowledge about EC2 Auto Scaling.\nStudied additional AWS storage options: EFS and FSx.\nExplored Lightsail and MGN for lightweight and migration use cases.\nWednesday (08/10/2025):\nGo to the office for studying.\nTranslated and edited an AWS blog.\nThursday (09/10/2025):\nExplored AWS Storage Services in depth:\nAmazon S3: Created buckets, uploaded files, configured ACLs, Bucket Policies, CORS, and Static Website Hosting.\nAmazon Storage Gateway: Connected on-premises data with AWS cloud storage.\nAWS Snow Family: Learned about large-scale data migration solutions.\nAWS Backup: Studied centralized data backup and recovery processes.\nDisaster Recovery (DR) on AWS: Understood strategies for resilience.\nCompleted and reviewed translated blog.\nFriday (10/10/2025):\nContinued studying Module 04: learned about Access Point and Storage Class in S3.\nPracticed Static Website Hosting and CORS configuration on S3.\nStudied Object Key, storage performance, and Amazon Glacier for long-term data archiving.\n"
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Continue watching AWS service tutorials on YouTube.\nPrepare knowledge for the midterm exam.\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch Module 5 – Shared Responsibility Model. - Study Amazon Identity and Access Management (IAM). 13/10/2025 13/10/2025 \u0026lt;Youtube: AWS Study Group\u0026gt; 3 - Continue Module 5: + Watch Amazon Identity and access management. + Watch Amazon Cognito. + AWS Organization. 14/10/2025 14/10/2025 \u0026lt;Youtube: AWS Study Group\u0026gt; 4 - Watch AWS Identity Center. - Watch Amazon Key Management Service. - Watch AWS Security Hub. 15/10/2025 15/10/2025 \u0026lt;Youtube: AWS Study Group\u0026gt; 5 - Study at the office. - Watch Module 6 - Database Concept + Amazon RDS \u0026amp; Amazon Aurora + Redshift \u0026amp; Elasticache - Optimize the project code. - Double-check translated blog articles. 16/10/2025 18/10/2025 \u0026lt;Youtube: AWS Study Group\u0026gt; 6 - Continue reviewing Module 6 labs. - Revise Modules 1–6 to prepare for the midterm exam. 18/10/2025 19/10/2025 \u0026lt;Youtube: AWS Study Group\u0026gt; Week 6 Achievements Monday (13/10/2025):\nWatched Module 5 – Shared Responsibility Model.\nStudied Amazon Identity and Access Management (IAM) to understand user and role permissions.\nTuesday (14/10/2025):\nContinued learning about IAM.\nStudied Amazon Cognito for authentication and user identity management.\nLearned AWS Organizations to manage multiple AWS accounts efficiently.\nWednesday (15/10/2025):\nStudied AWS Identity Center for centralized access management.\nExplored Amazon Key Management Service (KMS) for key creation, encryption, and security management.\nLearned about AWS Security Hub, a tool for continuous security monitoring.\nThursday (16/10/2025):\nStudied at the office and practiced using AWS services.\nWatch Module 6.\nCarefully checked translated AWS blogs for accuracy.\nFriday (17/10/2025):\nContinued with Module 6 labs for hands-on learning.\nReviewed Modules 1–6 thoroughly to prepare for the upcoming midterm exam.\nOptimized the project’s code.\n"
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Review AWS lab modules.\nStudy materials for the midterm exam.\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review Lab Module 1. - Review midterm exam topics. 20/10/2025 21/10/2025 \u0026lt;Youtube: AWS Study Group\u0026gt; 3 - Review Modules 1 and Lab 1. 21/10/2025 21/10/2025 \u0026lt;Youtube: AWS Study Group\u0026gt; 4 - Review Modules 2 and Lab 2. 22/10/2025 22/10/2025 \u0026lt;Youtube: AWS Study Group\u0026gt; 5 - Review Modules 3 \u0026amp; 4 and their labs. Go to the office for studying 23/10/2025 25/10/2025 \u0026lt;Youtube: AWS Study Group\u0026gt; 6 - Review Modules 5 \u0026amp; 6 and their labs. Go to the office for studying 24/10/2025 26/10/2025 \u0026lt;Youtube: AWS Study Group\u0026gt; Week 7 Achievements Monday (20/10/2025):\nReviewed Lab Module 1 to reinforce practical AWS concepts.\nReview midterm exam content.\nTuesday (21/10/2025):\nCompleted review of Module 1 and Lab 1.\nUnderstanding of AWS basics, including global infrastructure and shared responsibility.\nWednesday (22/10/2025):\nReviewed Module 2 and Lab 2.\nPracticed hands-on exercises related to VPC, subnets, and EC2 instances.\nThursday (23/10/2025):\nGo to the office for studying.\nReviewed Modules 3 \u0026amp; 4 and its labs.\nStrengthened understanding of S3, EBS, and Disaster Recovery concepts.\nFriday (24/10/2025):\nGo to the office for studying.\nReviewed Module 5 \u0026amp; 6 and its labs.\nConsolidated knowledge across Modules 1–6 in preparation for the midterm exam.\n"
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Midterm Exam Review this week. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review theory for Module 1-2. 27/10/2025 27/10/2025 \u0026lt;Youtube: AWS Study Group\u0026gt; 3 - Review theory for Module 3-4. 28/10/2025 29/10/2025 \u0026lt;Youtube: AWS Study Group\u0026gt; 4 - Review theory for Module 5-6. 29/10/2025 29/10/2025 \u0026lt;Youtube: AWS Study Group\u0026gt; 5 - Review all theory from 6 modules. 30/10/2025 30/10/2025 \u0026lt;Youtube: AWS Study Group\u0026gt; 6 - Go to the office for the Midterm Exam. 31/10/2025 31/10/2025 Week 8 Achievements: Monday (27/10/2025):\nReview Module 1–2. Tuesday (28/10/2025):\nReview Module 3–4. Wednesday (29/10/2025):\nReview Module 5–6. Thursday (30/10/2025):\nSummarize and consolidate all knowledge from Module 1–6. Friday (31/10/2025):\nMidterm Exam at the Office. "
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Review the structure of the project. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Read the project documentation. 03/11/2025 03/11/2025 3 - Review the project diagram. 04/11/2025 04/11/2025 4 - Identify the services required by the project. 05/11/2025 05/11/2025 5 - Determine how many services need to be applied. 06/11/2025 06/11/2025 6 - Review the overall structure of the project. 07/11/2025 07/11/2025 Week 9 Achievements: Monday (03/11/2025):\nRead through the project documentation to understand its purpose and scope. Tuesday (04/11/2025):\nReviewed the project diagram to visualize the system architecture. Wednesday (05/11/2025):\nExamined which services are required for the project implementation. Thursday (06/11/2025):\nDetermined the total number of services needed and how they integrate with the system. Friday (07/11/2025):\nReviewed the overall structure of the project to consolidate understanding. "
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Typically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and its basic services.\nWeek 2: Practicing with technologies and understanding basic AWS services.\nWeek 3: Understanding how to use AWS technologies and basic services, including both the Console and CLI.\nWeek 4: Creating an AWS account and working with AWS modules.\nWeek 5: Continuing AWS learning through YouTube resources.\nWeek 6: Exploring more AWS services via YouTube.\nWeek 7: Reviewing modules and preparing for the Midterm\nWeek 8: Comprehensive review and taking the Midterm exam\nWeek 9: Exploring the project structure and related services\nWeek 10: Reviewing and analyzing the project’s workflow\nWeek 11: Làm công việc M\u0026hellip;\nWeek 12: Làm công việc N\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "AWS Cloud Health Dashboard Comprehensive AWS Infrastructure Monitoring and Optimization Solution 1. Executive Summary. AWS Cloud Health Dashboard is a monitoring and optimization platform designed to help businesses and individuals effectively manage the costs, security, and performance of their AWS infrastructure.\nThe platform leverages a lightweight architecture built on EC2 and DynamoDB, integrating key AWS services such as CloudWatch, Cost Explorer, and Security Hub to deliver real-time monitoring, data visualization, and cost analytics.\nKey Highlights:\nOperational Cost: $12–18/month (leveraging AWS Free Tier). CloudWatch Integration: Real-time metrics tracking and alerting. Cost Analysis: Insights and optimization recommendations via AWS APIs. Real-Time Dashboard: Efficient data caching for fast performance. Scalable \u0026amp; Maintainable: Simple architecture for easy maintaining and scaling. 2. Problem Statement. Current Challenges:\nMany organizations and individual AWS users face several common issues:\nUncontrolled Costs:\nLack of centralized monitoring tools often leads to unexpected AWS billing spikes.\nLimited Security Visibility:\nNo unified dashboard to aggregate and visualize security findings.\nScattered Metrics:\nUsers must navigate across multiple AWS consoles to view performance and cost data.\nLack of Historical Data:\nCloudWatch retains metrics for only 15 days under the Free Tier.\nNo Custom Alerting:\nComplex or multi-condition alert configurations are difficult to implement.\nProposed Solution:\nCloud Health Dashboard delivers a centralized monitoring and optimization platform with the following key features:\nCentralized Monitoring:\nA single dashboard integrating CloudWatch, Cost Explorer, and Security Hub data.\nData Persistence with DynamoDB:\nFour dedicated tables: CloudHealthMetrics, CloudHealthCosts, SecurityFindings, and Recommendations. Automatic TTL policies for efficient data lifecycle management: Metrics: 30 days. Costs: 365 days. Security: 90 days. Recommendations: 180 days. On-Demand pricing to minimize cost. Optimized query patterns using GSIs for efficient access across data types. Cost analysis:\nHistorical cost trends. Service breakdown. AWS Cost Explorer recommendations integration. Budget alerts. Security monitoring:\nSecurity Hub findings aggregation. GuardDuty threat detection display. Compliance status tracking. Severity-based filtering. Intelligent recommendations:\nCost optimization suggestions. Performance improvements. Security enhancements. Impact-based prioritization. Performance Enhancements:\nRedis Caching to minimize AWS API calls and improve response time. Pre-Collected Data stored in DynamoDB for quick retrieval. WebSocket Integration for real-time data updates on the dashboard. Benefits and ROI. Cost Savings:\nFull visibility into underutilized or idle resources. Leverages AWS native cost recommendations (Cost Explorer, Trusted Advisor). Platform operating cost: only $12–18/month. Potential cost reduction: 15–25% by identifying and eliminating waste. Enhanced Visibility:\nUnified single dashboard replacing 5+ separate AWS consoles. Historical data retention beyond CloudWatch limits. Custom alerts and real-time notifications for faster response. Comprehensive view of performance, cost, and security metrics. Improved Productivity:\nReduces daily monitoring time from 30 minutes to just 5 minutes. Automated data collection and report generation. Proactive alerts prevent downtime and overspending. Actionable recommendations streamline operations and decision-making. 3. Solution Architecture. Architecture Overview:\nThe platform leverages a Single EC2 Instance + DynamoDB architecture to optimize cost and simplify deployment.\nAll application components (backend, API, and dashboard) run on a single EC2 instance, while DynamoDB is used for persistent and scalable data storage.\nRaw Design Draft:\nAWS Services Used:\nAmazon EC2 (Compute):\nt3.micro instance (750h/month Free Tier). Single public subnet (no NAT Gateway needed). Components: Nginx, FastAPI, Redis, React, CloudWatch Agent. Security: Restrictive security groups, Systems Manager Session Manager. Amazon DynamoDB (Storage):\n4 specialized tables: Metrics, Costs, Security, Recommendations. On-demand pricing mode. TTL enabled for automatic cleanup. Global Secondary Indexes for query optimization. Point-in-time recovery for backups. Amazon CloudWatch:\nMetrics collection with EC2, RDS, S3, Lambda, etc. Custom metrics for application monitoring. Logs aggregation. Alarms và notifications. AWS Cost Explorer:\nCost and usage data via API. Rightsizing recommendations (AWS native). Cost forecast (AWS native). Service breakdown. AWS Security Hub:\nSecurity findings aggregation. Compliance checking. Integration with GuardDuty. Amazon GuardDuty (Optional):\nThreat detection. Anomaly monitoring. Amazon S3 (Backup):\nDynamoDB backup exports. CloudWatch logs archive. Single Public Subnet Design:\nThe EC2 instance is deployed in a public subnet with the following security measures:\nSecurity Group Configuration:\nInbound: Port 80 and 443 open to 0.0.0.0/0. Inbound: Port 22 allowed only from trusted IPs (or fully disabled). Outbound: Unrestricted (to allow AWS API calls). Access Management:\nAWS Systems Manager Session Manager used instead of SSH. IAM Roles designed with the least privilege principle. No hardcoded credentials in the source code. Monitoring \u0026amp; Logging:\nVPC Flow Logs enabled for traffic visibility. CloudWatch Alarms configured for security events. GuardDuty enabled for continuous threat detection. Architecture Decision:\nA private subnet architecture was considered but not implemented due to:\nIncreased cost: Additional $33/month for NAT Gateway or ~$14/month for VPC Endpoints. Not suitable for a learning/portfolio project scope. Public subnet with proper security best practices is sufficient for this use case. Demonstrates cost-aware decision making. This architecture is appropriate for development or demo environments.\nFor production deployments, private subnets with NAT Gateways or VPC Endpoints should be used to enhance network isolation and security.\nComponents trong EC2 Instance:\nNginx (Port 80/443)\nReverse proxy và web server. Serve React static files. Proxy API requests to FastAPI. SSL/TLS termination. Gzip compression. FastAPI (Port 8000)\nRESTful API backend. AWS SDK integration (boto3). Business logic processing. Authentication/Authorization. Background task scheduling. Redis (Port 6379)\nCache AWS API responses. Cache DynamoDB query results. Session storage. Rate limiting. Typical cache hit rate: 60-80%. React Frontend\nSingle Page Application (SPA). Dashboard visualization. API client. Real-time updates via polling/WebSocket. Responsive design. CloudWatch Agent\nEC2 metrics collection. Application logs shipping. Custom metrics publishing. DynamoDB Table Design (4 Tables):\nTo optimize performance, data scalability, and separation of concerns, the system uses 4 specialized DynamoDB tables:\nTable 1: CloudHealthMetrics.\n{ \u0026#34;TableName\u0026#34;: \u0026#34;CloudHealthMetrics\u0026#34;, \u0026#34;KeySchema\u0026#34;: [ {\u0026#34;AttributeName\u0026#34;: \u0026#34;pk\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34;}, # service#metric_name {\u0026#34;AttributeName\u0026#34;: \u0026#34;sk\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34;} # ISO timestamp ], \u0026#34;AttributeDefinitions\u0026#34;: [ {\u0026#34;AttributeName\u0026#34;: \u0026#34;pk\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34;}, {\u0026#34;AttributeName\u0026#34;: \u0026#34;sk\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34;}, {\u0026#34;AttributeName\u0026#34;: \u0026#34;gsi1_pk\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34;} ], \u0026#34;GlobalSecondaryIndexes\u0026#34;: [ { \u0026#34;IndexName\u0026#34;: \u0026#34;MetricNameIndex\u0026#34;, \u0026#34;KeySchema\u0026#34;: [ {\u0026#34;AttributeName\u0026#34;: \u0026#34;gsi1_pk\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34;}, # metric_name {\u0026#34;AttributeName\u0026#34;: \u0026#34;sk\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34;} # timestamp ], \u0026#34;Projection\u0026#34;: {\u0026#34;ProjectionType\u0026#34;: \u0026#34;ALL\u0026#34;} } ], \u0026#34;TimeToLiveSpecification\u0026#34;: { \u0026#34;AttributeName\u0026#34;: \u0026#34;ttl\u0026#34;, \u0026#34;Enabled\u0026#34;: true # Auto-delete after 30 days }, \u0026#34;BillingMode\u0026#34;: \u0026#34;PAY_PER_REQUEST\u0026#34; } # Example item: { \u0026#34;pk\u0026#34;: \u0026#34;EC2#CPUUtilization\u0026#34;, \u0026#34;sk\u0026#34;: \u0026#34;2025-09-22T14:30:00Z\u0026#34;, \u0026#34;gsi1_pk\u0026#34;: \u0026#34;CPUUtilization\u0026#34;, # For cross-service queries \u0026#34;value\u0026#34;: 75.5, \u0026#34;unit\u0026#34;: \u0026#34;Percent\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;EC2\u0026#34;, \u0026#34;metric_name\u0026#34;: \u0026#34;CPUUtilization\u0026#34;, \u0026#34;dimensions\u0026#34;: {\u0026#34;InstanceId\u0026#34;: \u0026#34;i-1234567890abcdef0\u0026#34;}, \u0026#34;ttl\u0026#34;: 1669824000 # Unix timestamp } Table 2: CloudHealthCosts.\n{ \u0026#34;TableName\u0026#34;: \u0026#34;CloudHealthCosts\u0026#34;, \u0026#34;KeySchema\u0026#34;: [ {\u0026#34;AttributeName\u0026#34;: \u0026#34;pk\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34;}, # service {\u0026#34;AttributeName\u0026#34;: \u0026#34;sk\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34;} # date#granularity ], \u0026#34;AttributeDefinitions\u0026#34;: [ {\u0026#34;AttributeName\u0026#34;: \u0026#34;pk\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34;}, {\u0026#34;AttributeName\u0026#34;: \u0026#34;sk\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34;} ], \u0026#34;TimeToLiveSpecification\u0026#34;: { \u0026#34;AttributeName\u0026#34;: \u0026#34;ttl\u0026#34;, \u0026#34;Enabled\u0026#34;: true # Auto-delete after 365 days }, \u0026#34;BillingMode\u0026#34;: \u0026#34;PAY_PER_REQUEST\u0026#34; } # Example item: { \u0026#34;pk\u0026#34;: \u0026#34;EC2\u0026#34;, \u0026#34;sk\u0026#34;: \u0026#34;2025-09-22#DAILY\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2025-09-22\u0026#34;, \u0026#34;granularity\u0026#34;: \u0026#34;DAILY\u0026#34;, \u0026#34;cost\u0026#34;: 12.45, \u0026#34;usage_quantity\u0026#34;: 24.0, \u0026#34;usage_unit\u0026#34;: \u0026#34;Hrs\u0026#34;, \u0026#34;currency\u0026#34;: \u0026#34;USD\u0026#34;, \u0026#34;tags\u0026#34;: {\u0026#34;Environment\u0026#34;: \u0026#34;Production\u0026#34;}, \u0026#34;ttl\u0026#34;: 1704067200 # Unix timestamp (1 year) } Table 3: SecurityFindings.\n{ \u0026#34;TableName\u0026#34;: \u0026#34;SecurityFindings\u0026#34;, \u0026#34;KeySchema\u0026#34;: [ {\u0026#34;AttributeName\u0026#34;: \u0026#34;pk\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34;}, # finding_type {\u0026#34;AttributeName\u0026#34;: \u0026#34;sk\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34;} # finding_id ], \u0026#34;AttributeDefinitions\u0026#34;: [ {\u0026#34;AttributeName\u0026#34;: \u0026#34;pk\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34;}, {\u0026#34;AttributeName\u0026#34;: \u0026#34;sk\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34;}, {\u0026#34;AttributeName\u0026#34;: \u0026#34;gsi1_pk\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34;}, {\u0026#34;AttributeName\u0026#34;: \u0026#34;gsi1_sk\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34;} ], \u0026#34;GlobalSecondaryIndexes\u0026#34;: [ { \u0026#34;IndexName\u0026#34;: \u0026#34;SeverityIndex\u0026#34;, \u0026#34;KeySchema\u0026#34;: [ {\u0026#34;AttributeName\u0026#34;: \u0026#34;gsi1_pk\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34;}, # severity {\u0026#34;AttributeName\u0026#34;: \u0026#34;gsi1_sk\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34;} # created_at ], \u0026#34;Projection\u0026#34;: {\u0026#34;ProjectionType\u0026#34;: \u0026#34;ALL\u0026#34;} } ], \u0026#34;TimeToLiveSpecification\u0026#34;: { \u0026#34;AttributeName\u0026#34;: \u0026#34;ttl\u0026#34;, \u0026#34;Enabled\u0026#34;: true # Auto-delete after 90 days }, \u0026#34;BillingMode\u0026#34;: \u0026#34;PAY_PER_REQUEST\u0026#34; } # Example item: { \u0026#34;pk\u0026#34;: \u0026#34;GuardDuty\u0026#34;, \u0026#34;sk\u0026#34;: \u0026#34;finding-abc123def456\u0026#34;, \u0026#34;gsi1_pk\u0026#34;: \u0026#34;HIGH\u0026#34;, # For severity-based queries \u0026#34;gsi1_sk\u0026#34;: \u0026#34;2025-09-22T14:30:00Z\u0026#34;, \u0026#34;finding_id\u0026#34;: \u0026#34;finding-abc123def456\u0026#34;, \u0026#34;finding_type\u0026#34;: \u0026#34;GuardDuty\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;HIGH\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;ACTIVE\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Suspicious network traffic detected\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Unusual outbound traffic from EC2 instance to known malicious IP\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;EC2\u0026#34;, \u0026#34;resource_id\u0026#34;: \u0026#34;i-1234567890abcdef0\u0026#34;, \u0026#34;resource_type\u0026#34;: \u0026#34;Instance\u0026#34;, \u0026#34;recommendation\u0026#34;: \u0026#34;Review security group rules and investigate instance activity\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2025-09-22T14:30:00Z\u0026#34;, \u0026#34;updated_at\u0026#34;: \u0026#34;2025-09-22T14:30:00Z\u0026#34;, \u0026#34;ttl\u0026#34;: 1677484800 # Unix timestamp (90 days) } Table 4: Recommendations.\n{ \u0026#34;TableName\u0026#34;: \u0026#34;Recommendations\u0026#34;, \u0026#34;KeySchema\u0026#34;: [ {\u0026#34;AttributeName\u0026#34;: \u0026#34;pk\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34;}, # type {\u0026#34;AttributeName\u0026#34;: \u0026#34;sk\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34;} # created_at#rec_id ], \u0026#34;AttributeDefinitions\u0026#34;: [ {\u0026#34;AttributeName\u0026#34;: \u0026#34;pk\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34;}, {\u0026#34;AttributeName\u0026#34;: \u0026#34;sk\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34;}, {\u0026#34;AttributeName\u0026#34;: \u0026#34;gsi1_pk\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34;}, {\u0026#34;AttributeName\u0026#34;: \u0026#34;gsi1_sk\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;N\u0026#34;} ], \u0026#34;GlobalSecondaryIndexes\u0026#34;: [ { \u0026#34;IndexName\u0026#34;: \u0026#34;ImpactIndex\u0026#34;, \u0026#34;KeySchema\u0026#34;: [ {\u0026#34;AttributeName\u0026#34;: \u0026#34;gsi1_pk\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34;}, # impact#service {\u0026#34;AttributeName\u0026#34;: \u0026#34;gsi1_sk\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34;} # estimated_savings ], \u0026#34;Projection\u0026#34;: {\u0026#34;ProjectionType\u0026#34;: \u0026#34;ALL\u0026#34;} } ], \u0026#34;TimeToLiveSpecification\u0026#34;: { \u0026#34;AttributeName\u0026#34;: \u0026#34;ttl\u0026#34;, \u0026#34;Enabled\u0026#34;: true # Auto-delete after 180 days }, \u0026#34;BillingMode\u0026#34;: \u0026#34;PAY_PER_REQUEST\u0026#34; } # Example item: { \u0026#34;pk\u0026#34;: \u0026#34;cost\u0026#34;, \u0026#34;sk\u0026#34;: \u0026#34;2025-09-22T14:30:00Z#rec-789xyz\u0026#34;, \u0026#34;gsi1_pk\u0026#34;: \u0026#34;HIGH#EC2\u0026#34;, # For impact-based queries \u0026#34;gsi1_sk\u0026#34;: 50.00, # estimated_savings for sorting \u0026#34;rec_id\u0026#34;: \u0026#34;rec-789xyz\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cost\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Rightsize EC2 instance i-1234567890abcdef0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Instance runs at \u0026lt;10% CPU utilization. Consider downsizing from t3.large to t3.small\u0026#34;, \u0026#34;impact\u0026#34;: \u0026#34;HIGH\u0026#34;, \u0026#34;effort\u0026#34;: \u0026#34;LOW\u0026#34;, \u0026#34;confidence\u0026#34;: 0.92, \u0026#34;estimated_savings\u0026#34;: 50.00, \u0026#34;estimated_savings_period\u0026#34;: \u0026#34;monthly\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;EC2\u0026#34;, \u0026#34;resource_id\u0026#34;: \u0026#34;i-1234567890abcdef0\u0026#34;, \u0026#34;resource_type\u0026#34;: \u0026#34;Instance\u0026#34;, \u0026#34;current_config\u0026#34;: \u0026#34;t3.large\u0026#34;, \u0026#34;recommended_config\u0026#34;: \u0026#34;t3.small\u0026#34;, \u0026#34;action_steps\u0026#34;: [ \u0026#34;1. Create AMI backup\u0026#34;, \u0026#34;2. Stop instance\u0026#34;, \u0026#34;3. Change instance type\u0026#34;, \u0026#34;4. Restart and monitor\u0026#34; ], \u0026#34;implemented\u0026#34;: false, \u0026#34;created_at\u0026#34;: \u0026#34;2025-09-22T14:30:00Z\u0026#34;, \u0026#34;updated_at\u0026#34;: \u0026#34;2025-09-22T14:30:00Z\u0026#34;, \u0026#34;ttl\u0026#34;: 1685894400 # Unix timestamp (180 days) } Design Rationale. The decision to use 4 specialized tables instead of 2 aggregated tables is based on clear architectural and operational benefits:\n1. Query Performance Each table is optimized for a specific access pattern:\nMetrics → Time-series queries with high write throughput. Costs → Aggregation queries by service and date range. Security → Real-time filtering by severity level. Recommendations → Impact-based sorting and status tracking. This design minimizes query latency and avoids unnecessary data scans.\n2. Separation of Concerns Clear data ownership and lifecycle management. Different TTL (Time to Live) policies for each data type. Easier to maintain, debug, and evolve over time. 3. Scalability Each table scales independently as workloads grow. Avoids hot partitions caused by data spikes in one category. GSIs (Global Secondary Indexes) tailored to specific query needs. 4. Cost Management Enables granular cost tracking per data category. Flexible TTLs reduce storage cost for short-lived data. Uses on-demand billing, ideal for variable workloads. Trade-offs Slightly higher cost (~$2–3/month) compared to 2-table setup. Slightly more complexity in deployment and maintenance. However, gains include better performance, cleaner architecture, and simpler scalability. 4. Technical Implementation. Technology Stack.\nBackend.\nPython 3.9+ with FastAPI. boto3 for AWS SDK. Redis for caching. uvicorn ASGI server. asyncio for asynchronous operations. Frontend.\nReact 18 with Vite. TanStack Query (React Query) for data fetching. Recharts for data visualization. Tailwind CSS for styling. Axios as HTTP client. Database.\nDynamoDB as the primary database (4 specialized tables). Redis as in-memory cache. DevOps.\nNginx as reverse proxy. systemd for service management. CloudWatch Agent for monitoring. Bash scripts for automation. Core Features Implementation.\nMetrics Collection Service.\nBackground job runs every 5 minutes. Collects metrics from CloudWatch API. Persists data to the CloudHealthMetrics table. Caches recent metrics in Redis (TTL = 5 minutes). Cost Analysis Service.\nDaily job collects cost data from Cost Explorer API. Stores historical cost records in CloudHealthCosts table. Generates charts and trend data for the dashboard. Surfaces AWS native recommendations (Cost Explorer, Trusted Advisor). Security Monitoring.\nPolls Security Hub and GuardDuty findings. Persists findings into the SecurityFindings table. Displays active findings with severity filtering. Triggers alerts for high/critical severity items. Read-only integration to avoid altering customer security state. Recommendations Engine.\nAnalyzes metrics, cost, and security datasets. Generates actionable recommendations. Stores suggestions in the Recommendations table. Prioritizes recommendations by impact and confidence. Tracks implementation status. API Endpoints.\nGET /api/v1/metrics - Retrieve metrics from DynamoDB. GET /api/v1/costs - Cost data and trends. GET /api/v1/security - Security findings. GET /api/v1/recommendations - Optimization recommendations. GET /api/v1/health - Health check. WS /ws - WebSocket cho real-time updates (optional). Caching Strategy.\nRedis cache for frequent queries. 5-minute TTL for metrics data. 1-hour TTL for cost data. 30-minute TTL for security findings. 2-hour TTL for recommendations. Cache invalidation on data refresh. Security Measures\nHTTPS only with Let\u0026rsquo;s Encrypt. IAM roles with least privilege. Security groups restrictive rules. No hardcoded credentials. VPC Flow Logs enabled. 5. Roadmap \u0026amp; Implementation Milestones. MVP Features (3 months, 4-member team):\nMONTH 1: Core Infrastructure \u0026amp; Basic Features.\r├─ WEEK 1: Infrastructure Setup (1 DevOps + 3 support).\r│ ├─ EC2 instance launch and configuration.\r│ ├─ DynamoDB tables creation (4 tables with GSIs).\r│ ├─ Security groups, IAM roles.\r│ ├─ Development environment setup.\r│ └─ Deliverable: Working infrastructure with 4 DynamoDB tables.\r│\r├─ WEEK 2: Backend Foundation (2 Backend + 2 Frontend start).\r│ ├─ FastAPI skeleton with basic endpoints.\r│ ├─ DynamoDB connection and basic CRUD for 4 tables.\r│ ├─ CloudWatch integration (read metrics).\r│ ├─ React app initialization.\r│ └─ Deliverable: API responding, Frontend routing.\r│\r├─ WEEK 3: Core Monitoring (2 Backend + 2 Frontend).\r│ ├─ Metrics collection background job.\r│ ├─ Store metrics in CloudHealthMetrics table.\r│ ├─ Basic dashboard page với charts.\r│ ├─ Display real CloudWatch data.\r│ └─ Deliverable: Metrics monitoring working.\r│\r└─ WEEK 4: Cost Integration (2 Backend + 2 Frontend).\r├─ Cost Explorer API integration.\r├─ Cost data storage in CloudHealthCosts table.\r├─ Cost dashboard page.\r├─ Charts and trends visualization.\r└─ Deliverable: Month 1 MVP - Metrics + Costs monitoring.\rMONTH 2: Additional Features \u0026amp; Polish.\r├─ WEEK 5: Redis Caching (1 Backend + 1 DevOps + 2 Frontend).\r│ ├─ Redis setup and integration.\r│ ├─ Cache layer implementation for 4 tables.\r│ ├─ Performance optimization.\r│ ├─ Frontend improvements.\r│ └─ Deliverable: Improved performance.\r│\r├─ WEEK 6: Security Monitoring (2 Backend + 2 Frontend).\r│ ├─ Security Hub and GuardDuty integration.\r│ ├─ Store findings in SecurityFindings table.\r│ ├─ Security dashboard page and severity filtering.\r│ ├─ Alert system basic.\r│ └─ Deliverable: Security visibility.\r│\r├─ WEEK 7: Recommendations Engine (2 Backend + 2 Frontend).\r│ ├─ AWS Cost Explorer recommendations integration.\r│ ├─ Rule-based cost optimization logic.\r│ ├─ Store in Recommendations table.\r│ ├─ Recommendations UI with impact sorting.\r│ ├─ Testing and bug fixes.\r│ └─ Deliverable: Optimization suggestions.\r│\r└─ WEEK 8: Integration \u0026amp; Testing (All 4).\r├─ End-to-end testing across 4 tables.\r├─ Bug fixes.\r├─ Performance tuning.\r├─ Documentation.\r└─ Deliverable: Stable application.\rMONTH 3: Production Ready \u0026amp; Deployment.\r├─ WEEK 9: Deployment Automation (1 DevOps + 3 support).\r│ ├─ SSL certificates setup.\r│ ├─ Nginx configuration.\r│ ├─ Systemd services.\r│ ├─ Monitoring setup.\r│ └─ Deliverable: Production deployment.\r│\r├─ WEEK 10: Documentation \u0026amp; Polish (All 4).\r│ ├─ User documentation.\r│ ├─ API documentation.\r│ ├─ DynamoDB schema documentation.\r│ ├─ Deployment guide.\r│ ├─ UI/UX improvements.\r│ └─ Deliverable: Production-ready system.\r│\r├─ WEEK 11: Testing \u0026amp; Bug Fixes (All 4).\r│ ├─ Load testing.\r│ ├─ Security testing.\r│ ├─ DynamoDB performance testing.\r│ ├─ Bug fixing.\r│ ├─ Performance optimization.\r│ └─ Deliverable: Stable production.\r│\r└─ WEEK 12: Demo Preparation (All 4).\r├─ Demo environment setup.\r├─ Presentation materials.\r├─ Final testing.\r├─ Knowledge transfer.\r└─ Deliverable: Project handover.\rPost-deployment: Maintenance \u0026amp; Enhancements.\r└─ Monitoring, bug fixes, feature requests. Phase 2 (Future Enhancements - 3-6 months after):\nMachine Learning cost prediction models. Multi-account support. Advanced analytics. Well-Architected Framework assessment. GuardDuty deep integration. AWS Config compliance tracking. Custom alerting workflows. Mobile responsive improvements. Advanced recommendation algorithms. 6. Budget Estimation. AWS Infrastructure Cost (Monthly):\nService Description Month 1 Month 2 Month 3 EC2 t3.micro 750h Free Tier $0 $0 $0 DynamoDB On-demand, 4 tables $3-4 $4-6 $6-8 CloudWatch Metrics + Logs (Free Tier) $0-1 $1-2 $2-3 Data Transfer 15GB Free Tier $0 $0-1 $1 S3 Backup storage (~5GB) $0 $0-1 $1 Security Services GuardDuty (optional) $0 $1 $1-2 TOTAL $3-5 $6-11 $11-18 Average 12-Month Cost: $12–18/month.\nDynamoDB Cost Breakdown (4 Tables):\nStorage: ~5GB total = $1.25/month.\nCloudHealthMetrics: 2GB ($0.50). CloudHealthCosts: 1GB ($0.25). SecurityFindings: 1GB ($0.25). Recommendations: 1GB ($0.25). Writes: ~800K requests/month = $1.00/month.\nMetrics: 500K writes ($0.625). Costs: 100K writes ($0.125). Security: 100K writes ($0.125). Recommendations: 100K writes ($0.125). Reads: ~4M requests/month = $2.00/month.\nEvenly distributed across 4 tables. GSI: Included in on-demand pricing.\nBackup: Free (Point-in-time recovery).\nTotal DynamoDB Cost: $4–7/month.\nNote: Costs may increase if:\nHigher metric collection frequency. More AWS services being monitored. Longer data retention periods. GuardDuty enabled (+$4–6/month). Increased security findings and recommendations. Cost Optimization Strategies:\nFully utilize AWS Free Tier (first 12 months). Use on-demand mode instead of provisioned capacity. Apply TTL for automatic data expiration per table. Use Redis caching to reduce DynamoDB reads. Set CloudWatch log retention to 7 days. Disable monitoring for unused AWS services. Perform batch writes where possible. Design efficient query patterns with GSIs. 7. Risk Assessment. Risk Matrix:\nRisk Impact Level Probability Overall Risk Budget overrun Medium Medium Medium EC2 downtime High Low Medium Scope creep Medium High High Technical complexity Medium Medium Medium AWS API rate limits Low Medium Low DynamoDB hot partitions Low Low Low Team coordination issues Medium Medium Medium Mitigation Strategies:\nCost Management\nAWS Budget alerts set at $15 and $20 thresholds. Daily cost monitoring in AWS Cost Explorer. DynamoDB on-demand mode (avoids unexpected bills). Monitor DynamoDB cost per table. Weekly cost review meetings. Kill switch to disable data collection if budget exceeds limits. EC2 Availability\nCloudWatch alarms for health checks. Systemd configured for automatic service restarts. Health check endpoint for uptime validation. Backup and recovery procedures documented. Target uptime: 98–99% (realistic for single-instance deployment). Scope Creep\nStrict MVP definition. Feature freeze after Week 8. Maintain a “Nice-to-Have” list for Phase 2. Weekly standups to track progress. Prioritize must-have features. Document 4-table design clearly for alignment. Technical Complexity\nBegin with the simplest viable solutions. Use AWS documentation extensively. Establish a code review process. Conduct technical spikes for unknown areas. Run DynamoDB data modeling workshops. Consult mentors when encountering blockers. API Rate Limits\nImplement exponential backoff on API retries. Aggressive Redis caching to reduce API calls. Monitor API usage metrics. Stay within AWS Free Tier limits. Use batch operations where possible. DynamoDB Hot Partitions\nProper partition key design. Write sharding for high-traffic tables. Monitor CloudWatch metrics for throttling. Efficient use of Global Secondary Indexes (GSI). Team Coordination\nDaily 15-minute standup meetings. Clear task ownership and accountability. Git workflow with feature branches and pull requests. Continuous documentation from Day 1. Shared knowledge base for all members. Maintain up-to-date DynamoDB schema documentation. Contingency Plan Service Failure\nSystemd auto-restart configuration. Manual recovery steps documented. Data Loss\nDaily backups to Amazon S3. DynamoDB Point-in-Time Recovery (PITR) enabled. Cost Spike\nImmediate budget alert notification. Manual review and throttle data collection. Schedule Delays\nDe-scope Phase 2 features. Focus resources solely on MVP delivery. DynamoDB Performance Issues\nFallback to direct AWS API calls. Reduce write frequency temporarily. 8. Expected Outcomes. Technical Deliverables:\nWorking Dashboard:\n4 main pages: Metrics, Costs, Security, Recommendations. Real data from AWS services. Responsive design. Basic authentication. Data Collection System:\nBackground jobs collecting metrics per 5 minutes. Cost data updated daily. Security findings updated hourly. Recommendations generated daily. Data stored in 4 specialized DynamoDB tables. Performance:\nCached response time: \u0026lt; 300ms (60-80% requests). Uncached response time: \u0026lt; 2 seconds. DynamoDB query time: 10-25ms (typical). GSI query time: 15-30ms. Target uptime: 98-99% (single instance). Cost Analysis:\nHistorical cost trends. Service breakdown charts. AWS Cost Explorer recommendations display. Budget alerts. Cost forecasting. Security Visibility:\nSecurity Hub findings dashboard. GuardDuty threat detection. Severity-based filtering using GSI. Real-time alerts cho critical findings. Compliance status overview. Recommendations System:\nCost optimization suggestions. Performance improvement recommendations. Security enhancement suggestions. Impact-based prioritization using GSI. Implementation tracking. Learning Outcomes:\nAWS Services:\nHands-on experience with EC2, DynamoDB (advanced), CloudWatch. Cost Explorer API integration. Security Hub and GuardDuty understanding. IAM roles and policies. VPC networking basics. DynamoDB data modeling best practices. Full-Stack Development:\nFastAPI backend development. React frontend development. RESTful API design. NoSQL database design patterns. DynamoDB access patterns và GSI optimization. Caching strategies. DevOps:\nLinux server administration. Nginx configuration. Service management với system. SSL/TLS setup. Monitoring và logging. Database backup strategies. Best Practices:\nSecurity-first design. Cost optimization. Code organization. Documentation. Testing strategies. NoSQL data modeling. Portfolio Value:\nProject này demonstrate:\nReal AWS production experience. Advanced DynamoDB data modeling (4 tables, GSI). Full-stack development skills. Cost-aware architecture decisions. Security consciousness. Realistic project scoping. Team collaboration. Professional documentation. Limitations \u0026amp; Trade-offs:\nDocumented limitations:\nSingle instance (no high availability). Public subnet (no private network isolation). Rule-based recommendations (no ML-powered initially). Manual AWS service additions. Limited to AWS (no multi-cloud). Documented trade-offs:\nUsing 4 tables increases complexity and cost ($2–3/month) but improves performance and maintainability. Hosting the EC2 instance in a public subnet reduces security but saves approximately $33/month. Operating with a single instance lowers availability but aligns with the project\u0026rsquo;s budget constraints. These limitations and trade-offs are conscious decisions made for cost efficiency and timeline feasibility, demonstrating real-world engineering judgment and prioritization under resource constraints.\nA. Links:\nGitHub Repository B. Contact:\nProject Leader: Truong Quoc Tuan . Email: unviantruong26@gmail.com . WhatsApp: 0798806545 . --- "
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Connect and get acquainted with members of First Cloud Journey.\nUnderstand basic AWS services, how to use the console \u0026amp; CLI.\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study at the office. - Review the project structure. 10/11/2025 10/11/2025 3 - Examine the details of the services used in the project. 11/11/2025 11/11/2025 4 - Analyze the overall system workflow. 12/11/2025 12/11/2025 5 - Check the relationships and dependencies between services. 13/11/2025 13/11/2025 6 - Summarize the week’s findings and note unclear points. 14/11/2025 14/11/2025 7 - Attending the AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS event at the office. 14/11/2025 14/11/2025 Week 10 Achievements: Monday (10/11/2025):\nStudied at the office as planned.\nReviewed the overall project structure and understood the main components and folder organization.\nTuesday (11/11/2025):\nExamined the specific services implemented in the project.\nTook notes on the function and purpose of each service for future reference.\nWednesday (12/11/2025):\nAnalyzed the overall system workflow.\nUnderstood how modules interact and how data flows throughout the system.\nThursday (13/11/2025):\nChecked the relationships between services.\nIdentified dependencies and potential areas for optimization.\nFriday (14/11/2025):\nSummarized all progress made during the week.\nNoted unclear areas to clarify in the upcoming week.\nSaturday (15/11/2025):\nAttending the AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS event at the office. "
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Execute the coding project. Attend AWS Events at the office. Tasks to be carried out this week: Day Task Start Date Completion Date Resources Mon - Attend the AWS Cloud Mastery Series #2: DevOps on AWS event at the office. 17/11/2025 17/11/2025 Tue - Review the modules assigned for coding in detail. 18/11/2025 18/11/2025 Wed - Begin coding the initial parts. 19/11/2025 19/11/2025 Thu - Attend the AWS for SAP Using Generative AI \u0026amp; SAP ABAP capabilities and Amazon Q Developer event at the office. 20/11/2025 20/11/2025 Fri - Continue coding. 21/11/2025 21/11/2025 Week 11 Achievements: Monday (17/11/2025):\nAttended the AWS Cloud Mastery Series #2: DevOps on AWS event at the office. Tuesday (18/11/2025):\nThoroughly reviewed the modules assigned by the leader for coding. Wednesday (19/11/2025):\nStarted coding the initial parts of the project. Thursday (20/11/2025):\nAttended the AWS for SAP Using Generative AI \u0026amp; SAP ABAP capabilities and Amazon Q Developer event at the office. Friday (21/11/2025):\nContinued coding the assigned modules. "
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Code the assigned modules. Leader assigned the tasks: System Foundation \u0026amp; Testing. Tasks to be Implemented This Week: Day Task Start Date Completion Date Resources Mon - Code the first assigned modules. 24/08/2025 25/08/2025 Tue - Continue coding. 25/08/2025 26/08/2025 Wed - Hand over the rough code to the leader for review and adjustments. 26/08/2025 27/08/2025 Thu - Begin Testing the completed modules. 27/08/2025 28/08/2025 Fri - Continue Testing. 28/08/2025 29/08/2025 Sat - Attend the AWS Cloud Mastery Series #3 event at the office: Following the AWS Well-Architected Security Pillar. 29/08/2025 29/08/2025 Week 12 Achieved Outcomes: Monday (24/11/2025):\nCoded the assigned modules. Tuesday (25/11/2025):\nContinued coding the assigned modules. Wednesday (26/11/2025):\nHanded over the rough code to the leader for review, correction, and supplementation within the code. Thursday (27/11/2025):\nPerformed Testing on the completed modules. Friday (28/11/2025):\nContinued Testing the modules. Saturday (29/11/2025):\nAttended the AWS Cloud Mastery Series #3: Following the AWS Well-Architected Security Pillar event at the office. "
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/3-translatedblog/",
	"title": "Blog Translated",
	"tags": [],
	"description": "",
	"content": "This section will list and introduce the blogs that I have translated.\nBlog 1 - AWS DMS validation: A custom serverless architecture. This blog presents how AWS designed a custom serverless architecture to automate and optimize the data validation process in AWS Database Migration Service (DMS). It provides a step-by-step guide on building a post-migration data verification system using AWS Lambda, Step Functions, S3, and DynamoDB — ensuring data integrity and accuracy between source and target. Additionally, the blog shares a reference architecture, implementation details of the validation workflow, and cost optimization tips for operating the system. Read full blogs on Google Docs\nBlog 2 - Transforming network operations with AI: How Swisscom built a network assistant using Amazon Bedrock. This blog describes how Swisscom — a major telecommunications provider in Europe — leveraged Amazon Bedrock to build an AI Network Assistant that supports network operations. The article explains how the system utilizes large language models (LLMs) to automatically analyze logs, diagnose issues, and suggest solutions for network engineers. In addition, it introduces the overall AI architecture, the model training and integration process, and the performance and accuracy improvements Swisscom achieved through Generative AI. Read full blogs on Google Docs\nBlog 3 - Introducing the latest AWS Well-Architected Framework: IoT Lens. This blog introduces the AWS Well-Architected IoT Lens, a new extension of the AWS Well-Architected Framework designed to help businesses architect and evaluate their IoT systems following AWS best practices. The article outlines the five core pillars — operational excellence, security, reliability, performance efficiency, and cost optimization — in the context of IoT, along with reference architectures, best practices, and automated assessment tools. It serves as a valuable resource for engineers aiming to ensure their IoT systems are secure, efficient, and scalable on the AWS platform. Read full blogs on Google Docs\n"
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/4-eventparticipants/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Reflection Report: “AWS Cloud Day Vietnam 2025 \u0026amp; GenAI Track” Purpose of the Event. Introduce the latest technology trends from AWS, especially Generative AI.\nShare case studies from major enterprises in Vietnam and across the region.\nDiscuss leadership strategies and organizational management in the AI era.\nExplore best practices for security and real-world applications of AI Agents.\nSpeaker List. Morning: Hon. Government Speaker – Opening remarks.\nEric Yeo – Country General Manager, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS.\nDr. Jens Lottner – CEO, Techcombank.\nMs. Trang Phung – CEO \u0026amp; Co-Founder, U2U Network.\nJaime Valles – Vice President, General Manager Asia Pacific and Japan, AWS.\nJeff Johnson – Managing Director, ASEAN, AWS (Moderator).\nPanelists:\nVu Van – Co-founder \u0026amp; CEO, ELSA Corp\nNguyen Hoa Binh – Chairman, Nexttech Group\nDieter Botha – CEO, TymeX\nAfternoon: Kien Nguyen – Solutions Architect, AWS.\nMichael Armentano – Principal WW GTM Specialist, AWS.\nJun Kai Loke – AI/ML Specialist SA, AWS.\nTamelly Lim – Storage Specialist SA, AWS.\nBinh Tran – Senior Solutions Architect, AWS.\nTaiki Dang – Solutions Architect, AWS.\nKey Contents. Part 1: Main Sessions (AWS Cloud Day Vietnam). Opening \u0026amp; Keynote (Eric Yeo, AWS): Overview of AWS’s vision in Vietnam and Southeast Asia.\nCustomer Keynote 1 (Techcombank): Applying AI in finance to enhance operational efficiency and customer experience.\nCustomer Keynote 2 (U2U Network): Combining blockchain and AI to build a decentralized ecosystem.\nAWS Keynote (Jaime Valles): Regional tech trends and AWS’s strategies to empower innovation.\nPanel Discussion (Jeff Johnson + CEOs):\nLeadership must drive a culture of innovation.\nGenAI as a strategic enabler, not just a tool.\nChange management is critical when integrating AI.\nPart 2: GenAI Track. Building a Unified Data Foundation on AWS for AI and Analytics Workloads (Kien Nguyen):\nStrategies to build scalable and unified data foundations for AI \u0026amp; analytics.\nUtilizing AWS services for ingestion, storage, processing, and governance.\nBuilding the Future: GenAI Adoption and Roadmap on AWS (Jun Kai Loke \u0026amp; Tamelly Lim):\nOverview of the vision, trends, and development roadmap of GenAI on AWS.\nIntroduction to AWS services and initiatives supporting enterprise adoption.\nAI-Driven Development Lifecycle (AI-DLC) Shaping the Future of Software Implementation (Binh Tran):\nIntegrating AI into the entire software development lifecycle.\nCombining AI-assisted execution with human oversight to improve speed and quality.\nSecuring Generative AI Applications (Taiki Dang):\nAddressing GenAI security challenges: infrastructure, models, applications.\nSolutions: encryption, zero-trust architecture, continuous monitoring, fine-grained access control.\nBeyond Automation: AI Agents (Michael Armentano):\nAI Agents as “intelligent partners” beyond simple automation.\nCapable of learning, adapting, and executing complex tasks → driving exponential productivity.\nKeynotes. Eric Yeo (AWS): Vision for GenAI and its applications in Southeast Asia.\nTechcombank (Dr. Jens Lottner): Real-world AI adoption in financial services.\nU2U Network (Trang Phung): Synergy between blockchain and AI to build next-gen ecosystems.\nAWS (Jaime Valles): Regional innovation trends and enterprise transformation strategies.\nPanel Discussion: Navigating the GenAI Revolution. The leadership role in aligning organizations with GenAI.\nNurturing a culture of innovation and driving AI initiatives.\nManaging organizational change during AI integration.\nTechnical Insights \u0026amp; Best Practices. Securing Generative AI Applications (Taiki Dang):\nSecurity challenges in GenAI stacks (infrastructure, model, application).\nKey practices: encryption, zero-trust, continuous monitoring, fine-grained access.\nBeyond Automation: AI Agents (Michael Armentano):\nAI Agents as intelligent collaborators beyond traditional automation.\nSelf-learning and adaptive capabilities → significant productivity gains.\nKey Takeaways. Strategic Thinking. GenAI is not just a technology—it’s a catalyst for organizational transformation.\nLeadership must foster innovation and manage change effectively\nThe convergence of AI + Blockchain + Cloud opens new opportunities.\nTechnical \u0026amp; Security Knowledge. Security is fundamental for all GenAI applications.\nZero-trust architecture and continuous monitoring ensure integrity.\nUnderstanding security from infrastructure → model → application level is critical.\nLeveraging AI Agents. AI Agents surpass traditional automation through learning and decision-making.\nIntegration can significantly boost productivity and efficiency.\nApplications to Work. Propose internal workshops on AI Strategy \u0026amp; Change Management.\nPilot AI Agents in repetitive workflows to increase efficiency.\nReview AI workload security following zero-trust and encryption principles.\nIntegrate AI-driven insights into business data analytics.\nEvent Experience. Learning from Business Leaders. Case studies from Techcombank, U2U, ELSA, and Nexttech showcased multi-industry AI adoption.\nInsights into how CEOs handle organizational change and innovation.\nHands-on Technical Insights. Gained understanding of security architecture in GenAI deployments.\nRealized the transformative potential of AI Agents in next-level automation.\nNetworking \u0026amp; Collaboration. Engaged with AWS experts and industry executives.\nRecognized the importance of collaboration between business and technology in the GenAI era.\nLessons Learned. Leadership plays a key role in enabling GenAI adoption.\nBalancing security, operational efficiency, and innovation is crucial.\nAI Agents will become an inevitable part of future workflows.\nEvent Photos. "
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/4-eventparticipants/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "EVENT REPORT: AI/ML/GenAI on AWS Date: Saturday, November 15, 2025.\nTime: 8:00 AM – 11:30 AM.\nLocation: AWS Vietnam Office.\nEvent Objective: To provide foundational to advanced knowledge on AI/ML and Generative AI (GenAI) services on the Amazon Web Services (AWS) platform.\n1. Welcome \u0026amp; Context (8:00 – 8:30 AM) The event kicked off with participant registration and networking among professionals and attendees.\nAn overview of learning objectives was provided, directing the content to focus on practical AWS tools.\nAn introduction to the AI/ML landscape in Vietnam highlighted key trends and major challenges in the region.\n2. AWS AI/ML Platform Overview (8:30 – 10:00 AM) This segment focused on Amazon SageMaker, the end-to-end Machine Learning (ML) platform from AWS:\nML Lifecycle: Attendees learned about the crucial stages, from data preparation and labeling to efficient model training, tuning, and deployment.\nIntegrated MLOps: Introduction to the built-in MLOps capabilities in SageMaker, which help automate and manage the model lifecycle in production environments.\nLive Demo: SageMaker Studio: A live demonstration of SageMaker\u0026rsquo;s integrated development environment (IDE), illustrating how data scientists can work effectively on the platform.\n10:00 – 10:15 AM | Coffee Break (15 minutes) 3. Generative AI (GenAI) with Amazon Bedrock (10:15 AM – 11:30 AM) After the coffee break, the focus shifted to GenAI via Amazon Bedrock, the hub for Foundation Models (FMs):\nFoundation Models: Analysis and comparison of leading FMs like Claude, Llama, and Titan, along with guidance on selecting the appropriate model for specific tasks.\nPrompt Engineering: Learning essential and advanced techniques, including Chain-of-Thought reasoning and Few-shot learning to optimize FM output.\nRetrieval-Augmented Generation (RAG): Introduction to the RAG architecture and the role of integrating a Knowledge Base to feed the FM proprietary information, enhancing accuracy.\nBedrock Agents \u0026amp; Guardrails: Exploration of how Bedrock Agents enable the creation of automated multi-step workflows. Concurrently, learning about Guardrails to ensure safety and content filtering against harmful outputs.\nLive Demo: A practical demonstration of building a simple GenAI chatbot using Bedrock components.\n11:30 AM | Finish \u0026amp; Lunch Lunch Break (Self-arranged) GENERAL ASSESSMENT AND NEXT STEPS Assessment:\nThe morning session successfully provided a strong foundation for both pillars of AI on AWS: SageMaker as the tool for traditional/custom ML, and Bedrock as the center for rapid GenAI adoption. The practical demos helped attendees clearly visualize the application capabilities of these services.\nKey Knowledge Acquired: Understanding the end-to-end ML process on SageMaker and the core components of GenAI (FMs, RAG, Agents) within Bedrock.\nProposed Next Steps:\nHands-on with Bedrock: Begin experimenting with Prompt Engineering using one of the Foundation Models in Bedrock (e.g., Claude or Titan) to build a prototype application.\nExplore SageMaker: Delve deeper into the MLOps features of SageMaker, focusing on automating the deployment and monitoring of models.\nEvent Photos. "
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/4-eventparticipants/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "EVENT REPORT: FOCUS ON AWS BEDROCK AGENT AND AGENTIC WORKFLOW Event Objective: To provide in-depth knowledge on building and operating automated workflows (Agentic Workflows) using AWS Bedrock Agent and to introduce optimization solutions from the partner, CloudThinker.\nPROGRAM SUMMARY Time Topic Speaker Key Highlights 9:00 - 9:10 Opening Nguyen Gia Hung, Head of Solutions Architect Set the goals and context for the event. 9:10 - 9:40 AWS Bedrock Agent Core Kien Nguyen, Solutions Architect Analysis of the architecture, core features, and operational mechanism of Bedrock Agent. 9:40 - 10:00 [Use Case] Building Agentic Workflow on AWS Viet Pham, Founder cum CEO Presented a real-world example of deploying automated workflows on AWS. 10:00 - 10:10 CloudThinker Introduction Thang Ton, Co-founder \u0026amp; COO Introduced the company and its solution focused on GenAI optimization. 10:10 - 10:40 CloudThinker Agentic Orchestration, Context Optimization on Amazon Bedrock (L300) Henry Bui, Head of Engineering Deep dive into Agent coordination techniques and context optimization for Bedrock. 10:40 - 11:00 Tea Break \u0026amp; Networking Opportunity for discussion and relaxation. 11:00 - 12:00 CloudThinker Hack: Hands-on Workshop Kha Van Direct practical session on building an Agentic solution. 12:00 Networking \u0026amp; Lunch Buffet IN-DEPTH KNOWLEDGE GAINED 1. Core Architecture of AWS Bedrock Agent The presentation by Mr. Kien Nguyen clarified the role of Bedrock Agent as a powerful tool to automate complex tasks by connecting Foundation Models (FM) with business systems and APIs.\nOperational Mechanism: The Agent uses the FM\u0026rsquo;s reasoning capability to analyze the user\u0026rsquo;s request, determine the necessary steps, and call Tools (Action Groups) to complete the task.\nKey Benefit: Reduces the programming effort for multi-step workflows and ensures safety (safety guardrails) when accessing internal data or systems.\n2. Practical Application: Building Agentic Workflows The Use Case from Mr. Viet Pham illustrated how to transform manual, sequential processes into automated, Agent-driven workflows.\nThe crucial point is the clear definition of system capabilities and APIs so the Agent can accurately understand and utilize them.\n3. Optimizing Agentic Workflows with CloudThinker CloudThinker\u0026rsquo;s session focused on addressing advanced (L300) challenges when operating Agents:\nAgentic Orchestration: Coordination strategies to manage multiple Agents or complex steps, ensuring consistency and performance.\nContext Optimization: Techniques for optimizing the context passed to the FM (e.g., RAG optimization, context window management) to reduce token costs and improve the Agent\u0026rsquo;s accuracy/reasoning capability.\nThis is key to deploying cost-effective and reliable Agentic applications in an enterprise environment.\n4. In-Depth Hands-on Workshop The CloudThinker Hack session, led by Kha Van, provided a valuable opportunity for attendees to:\nDirectly configure and deploy a simple Agent on Amazon Bedrock.\nApply optimization techniques for context and orchestration from CloudThinker to the built workflow.\nThe practical session reinforced theoretical knowledge and provided a better understanding of the technical challenges when moving Agents to production.\nASSESSMENT AND NEXT STEPS Assessment:\nThe event successfully delivered a deep dive, from foundational to advanced (L300), into one of the most powerful features of AWS GenAI: the Bedrock Agent. The combination of core AWS knowledge and optimization solutions from the partner, CloudThinker, offered high practical value, particularly through the Hands-on Workshop.\nProposed Next Steps:\nExperiment with Bedrock Agent: Build a simple Agent to automate a small business task (e.g., querying data from DynamoDB or calling an internal API) to solidify knowledge of Action Groups.\nResearch Context Optimization: Delve deeper into advanced context optimization and prompt engineering techniques (as presented by CloudThinker) for application in existing GenAI projects.\nExplore MLOps for Agents: Investigate how to monitor and manage the lifecycle of a Bedrock Agent (AgentOps), including updating Tools and Foundation Models.\nEvent Photos. "
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/4-eventparticipants/4.4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "EVENT REPORT: DEVOPS ON AWS Date: Monday, November 17, 2025\nTime: 8:30 AM – 5:00 PM\nLocation: AWS Office\nEvent Objective: To provide comprehensive knowledge on DevOps Culture, principles, performance metrics, and the suite of AWS DevOps services for building CI/CD pipelines, IaC, and Observability.\nMORNING SESSION (8:30 AM – 12:00 PM): CI/CD \u0026amp; INFRASTRUCTURE AS CODE 1. Welcome \u0026amp; DevOps Mindset (8:30 – 9:00 AM) The event began with a quick recap from the previous AI/ML session, setting the context for moving models/applications into production. The focus was on DevOps culture and principles, emphasizing collaboration between Development (Dev) and Operations (Ops). Introduction to key performance metrics such as DORA (Deployment Frequency, Lead Time for Changes, MTTR, Change Failure Rate) and MTTR (Mean Time To Recovery). 2. AWS DevOps Services – Building the CI/CD Pipeline (9:00 – 10:30 AM) This section delved into the AWS CodeFamily suite of tools to automate Continuous Integration (CI) and Continuous Deployment (CD):\nSource Control: Using AWS CodeCommit and discussing source code management strategies like GitFlow and Trunk-based. Build \u0026amp; Test: Configuring CodeBuild to automate compilation and running testing pipelines. Deployment: Analyzing advanced deployment strategies with CodeDeploy, including Blue/Green, Canary, and Rolling updates. Orchestration: Utilizing CodePipeline to automate the entire process from commit to production. Demo: A visual demonstration of a complete CI/CD pipeline on AWS. 3. Infrastructure as Code (IaC) (10:45 AM – 12:00 PM) IaC is the foundation of modern DevOps, ensuring environment consistency and reproducibility:\nAWS CloudFormation: Learning about Templates, Stacks, and how to use Drift Detection to identify discrepancies between the actual state and the source code. AWS CDK (Cloud Development Kit): Introducing CDK as a more modern approach, using familiar programming languages (TypeScript, Python,\u0026hellip;) to define infrastructure via Constructs and reusable patterns. Demo \u0026amp; Discussion: Demonstrating infrastructure deployment using both CloudFormation and CDK, along with a discussion on criteria for choosing the right IaC tool. AFTERNOON SESSION (1:00 PM – 5:00 PM): CONTAINER, OBSERVABILITY \u0026amp; BEST PRACTICES 4. Container Services on AWS (1:00 – 2:30 PM) Docker Fundamentals: Review of basic concepts related to Microservices and Containerization. Amazon ECR (Elastic Container Registry): The service for storing container images, including image scanning features and lifecycle policies. Amazon ECS \u0026amp; EKS: Comparing the two main container orchestration services: ECS (simpler, AWS integrated) and EKS (Kubernetes-based). Analysis of deployment and scaling strategies. AWS App Runner: A simplified solution for container deployment, focusing on code over infrastructure. Demo \u0026amp; Case Study: Illustrating microservices architecture deployment and comparing the services. 5. Monitoring \u0026amp; Observability (2:45 – 4:00 PM) CloudWatch: The core tool for collecting metrics, logs, alarms, and dashboards across the entire AWS ecosystem. AWS X-Ray: Provides Distributed Tracing to track request flow across microservices, helping to identify bottlenecks and performance issues. Demo: Setting up a comprehensive observability system (Full-stack observability). Best Practices: Best practices for Alerting, dashboards, and the on-call process. 6. DevOps Best Practices \u0026amp; Case Studies (4:00 – 4:45 PM) Advanced Deployment Strategies: Discussion on Feature flags and A/B testing within the deployment pipeline. Automated testing and deep integration with CI/CD. Incident Management: Managing incidents and the importance of Postmortems for learning and improvement. Case Studies: Lessons learned from startups and large enterprises that have undergone successful DevOps transformations. 7. Q\u0026amp;A \u0026amp; Wrap-up (4:45 – 5:00 PM) Addressing in-depth questions. Providing information on DevOps career pathways and relevant AWS certifications. GENERAL ASSESSMENT AND APPLICATION Assessment: The event covered the entire DevOps lifecycle, from culture, CI/CD process, infrastructure management as code, to container operations and observability. The knowledge delivered was clear, combining theory (DevOps Principles, DORA metrics) and practical tools (CodePipeline, CDK, X-Ray).\nKey Knowledge Acquired:\nMastery of the AWS CodeFamily tools for setting up automated CI/CD pipelines. Clear understanding of the pros and cons of CloudFormation and CDK for IaC. Ability to differentiate and apply container services (ECS, EKS). Setting up proactive monitoring systems using CloudWatch and X-Ray. Proposed Next Steps:\nPractice CI/CD: Utilize CodeCommit, CodeBuild, and CodePipeline to automate deployment for the current project (if applicable) or a sample application. Explore CDK: Start using AWS CDK to define project infrastructure instead of the Console, focusing on creating simple Constructs. Implement Observability: Integrate CloudWatch Logs and experiment with AWS X-Ray to trace function calls (API calls) within the project to apply monitoring principles from the start. Event Photos. "
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/4-eventparticipants/4.5-event5/",
	"title": "Event 5",
	"tags": [],
	"description": "",
	"content": "EVENT REPORT: THE AWS WELL-ARCHITECTED SECURITY PILLAR Event: AWS Cloud Mastery Series #3: Following the AWS Well-Architected Security Pillar.\nDate: Saturday, November 29, 2025.\nTime: 8:30 AM – 12:00 PM (GMT+7).\nLocation: AWS Vietnam Office, Bitexco Financial Tower, District 1, Ho Chi Minh City.\nEvent Objective: To provide in-depth knowledge on the 5 pillars of the Security Pillar within the Well-Architected Framework, including core principles, services, and defense strategies.\nPROGRAM SUMMARY Time Topic Key Focus 8:30 – 8:50 Opening \u0026amp; Security Foundation Role of the Security Pillar, Shared Responsibility Model, and top cloud threats in Vietnam. 8:50 – 9:30 Pillar 1: Identity \u0026amp; Access Management (IAM) Modern IAM Architecture (Users, Roles, Policies), IAM Identity Center, SCP, MFA, and Access Analyzer Demo. 9:30 – 9:55 Pillar 2: Detection Continuous monitoring with CloudTrail, GuardDuty, Security Hub, and the Detection-as-Code model. 10:10 – 10:40 Pillar 3: Infrastructure Protection Network security (VPC segmentation, SG vs NACL), WAF, Network Firewall, and Workload security. 10:40 – 11:10 Pillar 4: Data Protection Encryption (at-rest \u0026amp; in-transit), Key Management (KMS), Secrets Management (Secrets Manager), and Data Classification. 11:10 – 11:40 Pillar 5: Incident Response AWS IR lifecycle, Playbooks (Key Compromise, S3 Exposure), and automated response (Auto-response). 11:40 – 12:00 Wrap-Up \u0026amp; Q\u0026amp;A Common pitfalls and the Security Specialty learning roadmap. IN-DEPTH KNOWLEDGE BY THE 5 PILLARS 1. Security Foundation \u0026amp; Core Principles Core Principles: Emphasis on the necessity of Least Privilege, Zero Trust (never trust, always verify), and Defense in Depth (layered defense). Shared Responsibility Model: Clarifying the boundary of responsibility: AWS is responsible for the security of the Cloud (infrastructure, physical), while the Customer is responsible for security in the Cloud (data, IAM, configuration). 2. Pillar 1: Identity \u0026amp; Access Management (IAM) Modern Architecture: Avoid using long-term credentials (long-lived Access Keys) for users. Prioritize Roles for services and IAM Identity Center (SSO) for users. Multi-Account Control: Apply Service Control Policies (SCPs) at the AWS Organizations level and Permission Boundaries to set maximum limits for delegated permissions. Mini Demo: Demonstration of using Access Analyzer and the policy simulator tool to verify and validate access rights before deployment. 3. Pillar 2: Detection Continuous Monitoring: Utilize CloudTrail (at the Organization level) to log all API actions, GuardDuty for ML-powered abnormal threat detection, and Security Hub to aggregate findings. Detection-as-Code: Defining detection rules as code (e.g., Lambda, CloudFormation) to automate deployment and management. 4. Pillar 3: Infrastructure Protection Network Segmentation: Separate application tiers (Web, App, DB) using VPC segmentation. Clearly distinguish the roles of Security Groups (stateful) and NACLs (stateless). Perimeter Defense: Deploy WAF (Web Application Firewall) and Shield to protect applications against DDoS and Layer 7 attacks. 5. Pillar 4: Data Protection Encryption: Ensure data is encrypted both at-rest (stored in S3, EBS, RDS) and in-transit (transmitted via TLS/SSL). Key and Secrets Management: Use KMS (Key Management Service) to manage primary encryption keys, control key policies, and key rotation. Use Secrets Manager to store and automatically rotate secrets (database passwords, API keys). 6. Pillar 5: Incident Response IR Lifecycle: Guidance on adhering to the AWS standard lifecycle (Prepare, Detect, Respond, Recover). Response Automation: Develop automated Playbooks using Lambda or Step Functions for common incidents like compromised IAM keys or EC2 malware detection. Key steps include Snapshot, isolation, and evidence collection. ASSESSMENT AND NEXT STEPS Assessment:\nThe event provided a detailed and comprehensive map of security on AWS, going beyond individual services to connect them within a holistic architectural framework (Well-Architected). The knowledge is particularly valuable for developers/Ops to ensure that newly deployed systems adhere to the highest security standards (Zero Trust).\nProposed Next Steps:\nCurrent Project Audit: Apply the Security Pillar checklist to assess the project currently being coded/deployed. Focus especially on IAM (Least Privilege) and Data Protection (Encryption).\nImplement Detection-as-Code: Integrate CloudTrail and GuardDuty into the development/testing environment. Begin defining alerting rules as code for anomalous behavior.\nAdvanced Learning: Research the AWS Security Specialty Certification to consolidate knowledge on the 5 security pillars.\nEvent Photos. "
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/4-eventparticipants/4.6-event6/",
	"title": "Event 6",
	"tags": [],
	"description": "",
	"content": "EVENT REPORT: AWS FOR SAP USING GENERATIVE AI Time: 1:00 PM - 5:00 PM.\nSpeaker: Nonthakorn Junthapol.\nTopic: AWS for SAP Using Generative AI \u0026amp; SAP ABAP capabilities and Amazon Q Developer.\nEvent Objective: To explore how Generative Artificial Intelligence (GenAI) can optimize and modernize the SAP environment, focusing on enhancing ABAP development productivity through Amazon Q Developer.\nCONTEXT AND GOALS OF GENAI IN SAP 1. Modernizing SAP on AWS The event established the foundation by affirming AWS\u0026rsquo;s role as the preferred cloud platform for SAP S/4HANA systems (including the RISE with SAP model). Discussion covered traditional challenges in the SAP environment: the complexity of Legacy ABAP code, the need for modernization (e.g., migrating to S/4HANA), and slow development speed. 2. The Role of Generative AI GenAI was introduced as a strategic tool to overcome these challenges, especially in code automation, refactoring, and data interaction. Services like Amazon Bedrock (or custom models on SageMaker) can be used to analyze unstructured SAP data (e.g., PO documents, Invoices) after extraction via AWS. DEEP DIVE: AMAZON Q DEVELOPER AND ABAP CAPABILITIES 3. Introducing Amazon Q Developer Amazon Q Developer is an AI assistant designed to accelerate software development, customized to understand the context of AWS infrastructure and APIs. A key highlight is its ability to interact using natural language to solve technical issues and generate code. 4. Boosting Productivity with SAP ABAP The core focus of the event delved into how Amazon Q directly supports ABAP developers:\nAmazon Q Capability Application in ABAP Value Proposition Code Generation Quickly creating ABAP code snippets for BAPIs, Function Modules, or standard ALV reports. Increases initial coding speed. Code Explanation Analyzing and explaining complex or sparsely used legacy ABAP code segments. Reduces maintenance time and code learning curve. Refactoring \u0026amp; Modernization Supporting the conversion of old ABAP code to more modern object-oriented (OOP) syntax and principles (e.g., preparing for S/4HANA). Lowers the risk and cost of system migration/upgrade. Troubleshooting Suggesting solutions and debugging guidance based on error messages in the SAP environment. Minimizes Mean Time To Recovery (MTTR). 5. Integration Mechanism and Security Amazon Q functions as an abstraction layer, accessing ABAP source code through authorized connections (potentially via AWS Connector tools or integration with the development environment). Data Security: It was emphasized that ABAP source code data is processed securely and is not used to train the underlying Amazon Q model, ensuring the privacy and security of enterprise intellectual property. GENERAL ASSESSMENT AND NEXT STEPS Assessment: The event outlined a clear path for the future of SAP ABAP development, shifting from a manual and time-consuming process to one supported by AI. Amazon Q Developer serves as a disruptive tool, helping SAP enterprises fully leverage the power of the AWS Cloud, not just at the infrastructure level but also in application and development layers.\nKey Knowledge Acquired: Understanding the direct, practical applications of Amazon Q Developer in accelerating ABAP modernization and standard development tasks.\nProposed Next Steps:\nPilot Amazon Q Developer: SAP development teams should pilot Amazon Q Developer in a sandbox environment to test its capabilities in generating, explaining, and refactoring standard ABAP code snippets. Refactoring Planning: Utilize Amazon Q to assess the complexity and risk level of legacy ABAP code, thereby creating a detailed plan for modernization or migration projects to S/4HANA (Greenfield/Brownfield). Integrate GenAI into Data Flow: Investigate how to use other AWS GenAI services (beyond Q Developer) to process SAP business data streams (e.g., automated summarization of large sales orders, classification of transaction errors). Event Photos. "
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/4-eventparticipants/",
	"title": "Event Participants",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in six events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AWS Cloud Day Vietnam 2025 \u0026amp; GenAI Track.\nTime: 09:00 - 18/09/2025.\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City.\nRole: Attendee.\nEvent 2 Event Name: AWS Cloud Mastery Series #1 : AI/ML/GenAI on AWS\nTime: 8:30 AM – 12:00 AM ; Saturday, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: Building Agentic AI \u0026amp; Context Optimization with Amazon Bedrock\nTime: 9:00 AM - 12:00 PM ; Friday, December 5, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: AWS Cloud Mastery Series #2 : DevOps on AWS\nTime: 8:30 AM – 5:00 PM ; Monday, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 5 Event Name: AWS Cloud Mastery Series #3 : Security on AWS\nTime: 8:30 AM – 12:00 AM ; Saturday, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 6 Event Name: AWS for SAP Using Generative AI : SAP ABAP capabilities and Amazon Q Developer\nTime: 13:00 PM – 5:00 PM ; Thursday, November 20, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Introduction to VPC Endpoints. VPC Endpoint is a virtual device within an Amazon VPC that enables compute resources (such as EC2, Lambda, ECS,\u0026hellip;) to securely communicate with AWS services without traversing the public Internet. These Endpoints are designed to be horizontally scalable, highly redundant, and ensure high availability, eliminating the risk of connection loss due to reliance on an Internet Gateway or NAT Gateway. Compute resources running within a VPC can access Amazon S3 via a Gateway Endpoint, while an Interface Endpoint (AWS PrivateLink) can be used for resources within the VPC or in an On-Premises data center to access AWS services privately. Workshop Overview. In this workshop, we will:\nLearn how to create, configure, and test two types of VPC Endpoints (Gateway and Interface). Set up a Hybrid Access model, where workloads in the VPC and On-Premises systems can both access Amazon S3 securely, privately, and without going over the Internet. Applying the following security policy layers (Security Layers): Endpoint Policy: Controls access permissions at the Endpoint level. Bucket Policy: Restricts access based on the source IP and the allowed network range. Perform access testing (Positive \u0026amp; Negative Testing) to verify the operation of each security layer and ensure that only valid sources can access data in S3. Implementation Steps. Phase 1: Environment Preparation: Step Action Role 1.1 Create a VPC (Like: Hybrid-VPC) and 2 Subnet (1 Public, 1 Private). Cloud Environment 1.2 Create a S3 Bucket (Like: s3://hybrid-secure-data-prod). Data Storage 1.3 Create a EC2 Instance in Subnet Private (simulating the VPC Workload). Test Subject 1 1.4 Create a EC2 Instance in Subnet Public (simulating the On-Premise System) and assign a specific source IP (assuming this IP comes via Direct Connect/VPN). Test Subject 2 Phase 2: Configure Interface Endpoint (For Hybrid Access): Step Action Purpose 2.1 Create an Interface VPC Endpoint for S3. Select the Service Name as com.amazonaws.region.s3. Establish a private connection for On-Premise. 2.2 Configure Private DNS in the VPC so traffic from On-Premise resolves the S3 name to the Endpoint\u0026rsquo;s private IP. Ensure traffic does not go out to the Internet. 2.3 Apply the following Endpoint Policy to the Interface Endpoint, only allowing access to arn:aws:s3:::hybrid-secure-data-prod*. Security Layer 1: Restrict the Buckets accessible via this Endpoint. JSON Policy: { \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowAccessToSpecificBucket\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::hybrid-secure-data-prod\u0026#34;, \u0026#34;arn:aws:s3:::hybrid-secure-data-prod/*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;DenyAllOtherBuckets\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::*\u0026#34; } ] } Phase 3: Configure Gateway Endpoint (For In-VPC Access). Step Action Purpose 3.1 Create a Gateway VPC Endpoint for S3. Ensure S3 access within the VPC without needing to go through the Internet. 3.2 Update Route Table: Associate the Endpoint with the Route Table of the Private Subnet. Direct S3 traffic within the VPC to the Gateway Endpoint. 3.3 Apply Endpoint Policy: Apply a simple policy that allows access, but only for specific IAM Roles/Users within the VPC. Security Layer 2: Additional access control mechanism. Phase 4: Enforce S3 Bucket Policy (Source Control). Step Active Purpose 4.1 Configure the S3 Bucket Policy for s3://hybrid-secure-data-prod. Security Layer 3: The final control layer, ensuring only trusted sources are allowed. 4.2 Use the aws:SourceIp condition to only allow access from the On-Premise CIDR Block (IP of the simulated EC2 in 1.4) and the Private Subnet CIDR Block (EC2 in VPC). Limit access to S3 from outside the allowed Hybrid sources. JSON Policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;RestrictAccessToHybridSources\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::hybrid-secure-data-prod\u0026#34;, \u0026#34;arn:aws:s3:::hybrid-secure-data-prod/*\u0026#34; ], \u0026#34;Condition\u0026#34;: { \u0026#34;NotIpAddress\u0026#34;: { \u0026#34;aws:SourceIp\u0026#34;: [ \u0026#34;SIMULATED_ON_PREM_IP/32\u0026#34;, \u0026#34;CIDR_CỦA_SUBNET_PRIVATE\u0026#34; ] } } } ] } Phase 5: Testing and Negative Testing. The goal of this phase is to verify the effectiveness of the security layers applied in the VPC Endpoint configuration and S3 Bucket Policy, ensuring that only valid sources (identified via IP and Endpoint Policy) have access permission.\nStep Access Source Testing Goal Expected Result 5.1 EC2 On-Premise*(Allowed IP)* Accesss3://hybrid-secure-data-prod SUCCESS: Allowed because the source IP and Bucket Policy match. 5.2 EC2 On-Premise*(Allowed IP)* Access a different S3 bucket FAILURE: Denied by the Interface Endpoint Policy. 5.3 EC2 trong VPC*(Subnet Private)* Accesss3://hybrid-secure-data-prod SUCCESS: Allowed because the source IP and Bucket Policy match. 5.4 EC2 bất kỳ*(có Public IP)* Accesss3://hybrid-secure-data-prod via the Internet FAILURE: Denied by the S3 Bucket Policy (NotIpAddress). Analysis of Test Results. Valid Access (5.1 \u0026amp; 5.3): Sources within the allowed IP list or from the Private Subnet were able to access normally through the secure Endpoints. Denied Access (5.2 \u0026amp; 5.4): Accesses outside the allowed IP or bucket scope were successfully denied by the Endpoint Policy or Bucket Policy layer, enforcing the Zero Trust Access principle. Conclusion. The testing confirmed that:\nThe Gateway and Interface Endpoints function correctly, routing internal traffic without traversing the Internet.\nThe multi-layered security policies (VPC Endpoint Policy + S3 Bucket Policy) are effective.\nThe Hybrid Access model is secured, with controlled access sources, adhering to AWS internal security principles.\n"
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Implementing Detailed Security and Access Control for Hybrid S3 Access Overview. Extended Lab Idea:\nThis lab focuses on applying Defense-in-Depth security layers to ensure that only authenticated and authorized sources can access S3 resources through private connections (PrivateLink/VPC Endpoint).\nLab Objectives:\nAfter completing this lab, you will be able to:\nConfigure an Interface VPC Endpoint to extend private S3 connectivity to an on-premises (simulated) environment. Implement an Endpoint Policy to allow access to a specific S3 bucket only. Apply an S3 Bucket Policy to restrict access based on the source IP address of the on-premises data center. Demonstrate that S3 access through endpoints is secured and strictly limited following the Least Privilege principle. Contents Workshop Overview Prerequisites Accessing S3 from VPC Accessing S3 from On-premises Datacenter VPC Endpoint Policies (optional) Resource Cleanup "
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/6-self-evaluation/",
	"title": "Self-Evaluation",
	"tags": [],
	"description": "",
	"content": "During my internship at [Amazon Web Service] from [September] to [December], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality. ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly. ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions. ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality. ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes. ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself. ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly. ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams. ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment. ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity. ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team. ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period. ☐ ✅ ☐ Needs Improvement. Strengthen discipline and strictly comply with the rules and regulations of the company or any organization.\nImprove problem-solving thinking.\nEnhance communication skills in both daily interactions and professional contexts, including handling situations effectively.\n"
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/7-feedback/",
	"title": "Feedback",
	"tags": [],
	"description": "",
	"content": " Here, you can freely share your personal opinions and experiences during the First Cloud Journey program to help the FCJ team improve any shortcomings based on the following categories:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What are you most satisfied with during your internship?\n→ The mentors and team admins are always ready to help and support.\nWhat do you think the company should improve for future interns?\n→\nIf you were to recommend this internship to your friends, would you do so? Why?\n→\nSuggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience?\nWould you like to continue this program in the future?\nOther comments (feel free to share):\n"
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/FCJ-Workshop-Official/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]